{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOgyEKZFt+BXucRPsrYoZlS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rohanreddyp298/Democratizing-Database-Access---Text-to-SQL/blob/main/Democratizing_Database_Access_with_Text_to_SQL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üöÄ Project: Democratizing Database Access with Text-to-SQL\n",
        "\n",
        "**Student:** Rohan\n",
        "**Course:** DAMG 7370 - Designing Advanced Data Architectures\n",
        "**Date:** February 9, 2026\n",
        "\n",
        "## 1. Executive Summary & Objective\n",
        "In the modern enterprise, the gap between data availability and data accessibility is a critical bottleneck. While organizations possess vast SQL databases, extracting insights requires specialized technical skills.\n",
        "\n",
        "**The Goal:** This assignment aims to bridge that gap by fine-tuning a **Small Language Model (SLM)** to act as a specialized \"Text-to-SQL\" interface.\n",
        "\n",
        "**The Approach:**\n",
        "* **Model:** `deepseek-coder-1.3b-instruct` (chosen for its pre-trained code understanding).\n",
        "* **Technique:** **QLoRA** (Quantized Low-Rank Adaptation) to efficiently fine-tune on a single T4 GPU.\n",
        "* **Dataset:** `b-mc2/sql-create-context` (15,000 samples).\n",
        "* **Target:** Achieve >40% Exact Match accuracy on unseen complex queries."
      ],
      "metadata": {
        "id": "wVcZAPhlklHT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Environment Setup & Dependency Management\n",
        "To run this pipeline on a standard Google Colab T4 GPU (16GB VRAM), we must leverage specific libraries for efficient training.\n",
        "\n",
        "**Key Libraries:**\n",
        "* `bitsandbytes`: Enables **4-bit quantization**, reducing model footprint from ~6GB to ~2GB VRAM.\n",
        "* `peft`: Implements **LoRA**, allowing us to freeze the base model and only train a small percentage of adapter parameters.\n",
        "* `trl`: Transformers Reinforcement Learning library, providing the `SFTTrainer` for supervised fine-tuning loops.\n",
        "\n",
        "*Note: Specific versions are pinned below to ensure compatibility with Colab's CUDA drivers.*\n"
      ],
      "metadata": {
        "id": "qdD7NEeXkseZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Data Preparation & Engineering\n",
        "A generic LLM cannot generate accurate SQL without context. If we ask \"Show me the sales,\" it doesn't know if the table is named `sales`, `orders`, or `transactions`.\n",
        "\n",
        "**Dataset Selection:**\n",
        "We utilize the `b-mc2/sql-create-context` dataset, which provides a triplet of:\n",
        "1.  **Question:** Natural language query.\n",
        "2.  **Context:** The SQL Schema (CREATE TABLE statement).\n",
        "3.  **Answer:** The correct SQL query.\n",
        "\n",
        "**Preprocessing Strategy:**\n",
        "We implement **Instruction Tuning** by formatting each sample into a strict prompt template. This forces the model to attend to the *Schema Context* before generating the *Response*.\n",
        "\n",
        "$$\\text{Prompt} = \\text{Instruction} + \\text{User Query} + \\text{Schema Context} \\rightarrow \\text{SQL Output}$$"
      ],
      "metadata": {
        "id": "jtrR8-C3k4Z_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mXcegl-CPxsx"
      },
      "outputs": [],
      "source": [
        "# 1. Install Dependencies\n",
        "# We unpin bitsandbytes to get the latest CUDA 12 support\n",
        "# We keep trl pinned to 0.8.6 to ensure your code runs without syntax errors\n",
        "!pip install -q -U bitsandbytes\n",
        "!pip install -q -U triton\n",
        "!pip install -q -U transformers==4.41.2 peft==0.11.1 datasets==2.19.1 trl==0.8.6 accelerate==0.30.1\n",
        "\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 2. Dataset Selection & Preprocessing\n",
        "print(\"Loading dataset...\")\n",
        "dataset = load_dataset(\"b-mc2/sql-create-context\", split=\"train[:15000]\")\n",
        "\n",
        "# 3. Data Cleaning & Formatting\n",
        "def format_instruction(sample):\n",
        "    return f\"\"\"### Instruction:\n",
        "You are a powerful SQL expert. Convert the following natural language question into a SQL query using the provided context.\n",
        "\n",
        "### Question:\n",
        "{sample['question']}\n",
        "\n",
        "### Context:\n",
        "{sample['context']}\n",
        "\n",
        "### Response:\n",
        "{sample['answer']}\n",
        "\"\"\"\n",
        "\n",
        "# Apply formatting\n",
        "dataset = dataset.map(lambda x: {\"text\": format_instruction(x)})\n",
        "\n",
        "# 4. Splitting (Train/Val/Test)\n",
        "train_test = dataset.train_test_split(test_size=0.2, seed=42)\n",
        "test_val = train_test['test'].train_test_split(test_size=0.5, seed=42)\n",
        "\n",
        "dataset_dict = {\n",
        "    'train': train_test['train'],\n",
        "    'validation': test_val['train'],\n",
        "    'test': test_val['test']\n",
        "}\n",
        "\n",
        "print(f\"Data Shapes: Train: {len(dataset_dict['train'])}, Val: {len(dataset_dict['validation'])}, Test: {len(dataset_dict['test'])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Model Architecture & Quantization\n",
        "We selected **DeepSeek-Coder-1.3B-Instruct** as our base model. Unlike general chat models (e.g., Llama-2-7B), DeepSeek is pre-trained on massive code repositories, giving it an inherent understanding of SQL syntax (`SELECT`, `GROUP BY`, `JOIN`).\n",
        "\n",
        "**QLoRA Configuration:**\n",
        "To fit this model into memory, we employ **4-bit Normal Float (NF4)** quantization.\n",
        "* **Computation:** Matrix multiplications occur in float16.\n",
        "* **Storage:** Weights are stored in 4-bit.\n",
        "* **Rank (r):** We target **all linear layers** (`q_proj`, `k_proj`, `v_proj`, `o_proj`, etc.) to maximize learning capacity.\n"
      ],
      "metadata": {
        "id": "41RyxnkMlDoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "\n",
        "# Model Selection: deepseek-coder-1.3b\n",
        "# Justification: It's fast, efficient, and fits easily on Colab T4 while being capable of SQL logic.\n",
        "model_id = \"deepseek-ai/deepseek-coder-1.3b-instruct\"\n",
        "\n",
        "# Quantization Config (4-bit loading for efficiency)\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16\n",
        ")\n",
        "\n",
        "# Load Model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "model.config.use_cache = False # Silence warnings for training\n",
        "\n",
        "# Load Tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "tokenizer.pad_token = tokenizer.eos_token # Fix padding for Llama models\n",
        "tokenizer.padding_side = \"right\""
      ],
      "metadata": {
        "id": "h_49ww51RxQG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Hyperparameter Optimization (Grid Search)\n",
        "To meet the \"Quality Score\" requirement, we do not rely on default settings. We conducted a grid search across three distinct configurations to optimize for **Validation Loss**.\n",
        "\n",
        "**Experimental Design:**\n",
        "1.  **Config 1 (Aggressive):** High Learning Rate ($2e^{-4}$), Low Rank ($r=8$). Testing rapid convergence.\n",
        "2.  **Config 2 (Deep):** Low Learning Rate ($2e^{-5}$), High Rank ($r=64$). Testing detailed feature extraction.\n",
        "3.  **Config 3 (Balanced):** Mid-range settings.\n",
        "\n",
        "*The code block below executes this search on a subset of data to identify the winner.*"
      ],
      "metadata": {
        "id": "4e2ebX6ilTxk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "from trl import SFTTrainer\n",
        "\n",
        "# Define 3 configurations to test\n",
        "hyperparams_list = [\n",
        "    {\"lr\": 2e-4, \"r\": 8, \"alpha\": 16, \"batch\": 4},\n",
        "    {\"lr\": 2e-5, \"r\": 64, \"alpha\": 128, \"batch\": 2},\n",
        "    {\"lr\": 5e-5, \"r\": 16, \"alpha\": 32, \"batch\": 4}\n",
        "]\n",
        "\n",
        "results = {}\n",
        "\n",
        "for i, params in enumerate(hyperparams_list):\n",
        "    print(f\"\\n--- Testing Configuration {i+1} ---\")\n",
        "\n",
        "    # 1. PEFT Config (LoRA)\n",
        "    peft_config = LoraConfig(\n",
        "    r=params[\"r\"],\n",
        "    lora_alpha=params[\"alpha\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
        ")\n",
        "\n",
        "    # 2. Training Arguments (Classic Stable Syntax)\n",
        "    args = TrainingArguments(\n",
        "        output_dir=f\"./results_run_{i+1}\",\n",
        "        num_train_epochs=1,\n",
        "        per_device_train_batch_size=params[\"batch\"],\n",
        "        gradient_accumulation_steps=4,\n",
        "        learning_rate=params[\"lr\"],\n",
        "        logging_steps=10,\n",
        "        evaluation_strategy=\"steps\",  # Standard name for this version\n",
        "        eval_steps=50,\n",
        "        save_strategy=\"no\",\n",
        "        fp16=True,\n",
        "        report_to=\"none\"\n",
        "    )\n",
        "\n",
        "    # 3. Trainer\n",
        "    trainer = SFTTrainer(\n",
        "        model=model,\n",
        "        train_dataset=dataset_dict['train'].select(range(200)),\n",
        "        eval_dataset=dataset_dict['validation'].select(range(50)),\n",
        "        peft_config=peft_config,\n",
        "        dataset_text_field=\"text\", # Explicitly allowed in 0.8.6\n",
        "        max_seq_length=1024,        # Explicitly allowed in 0.8.6\n",
        "        tokenizer=tokenizer,\n",
        "        args=args,\n",
        "    )\n",
        "\n",
        "    # Train\n",
        "    trainer.train()\n",
        "\n",
        "    # Log Final Eval Loss\n",
        "    final_loss = trainer.evaluate()['eval_loss']\n",
        "    results[f\"Config {i+1}\"] = final_loss\n",
        "    print(f\"Config {i+1} Final Loss: {final_loss}\")\n",
        "\n",
        "print(\"\\nHyperparameter Search Results:\", results)\n",
        "best_config_index = min(results, key=results.get)\n",
        "print(f\"Best Configuration is: {best_config_index}\")"
      ],
      "metadata": {
        "id": "RDpMGTAMTXE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Final Training Pipeline\n",
        "Based on the Hyperparameter Search results, **Config 2 (Rank 64, LR 2e-5)** demonstrated the best stability and lowest loss.\n",
        "\n",
        "We now execute the full training run on the complete dataset (15,000 examples).\n",
        "* **Epochs:** 3\n",
        "* **Batch Size:** 4 (with Gradient Accumulation steps = 2)\n",
        "* **Callbacks:** `EarlyStoppingCallback` is implemented to prevent overfitting if validation loss increases.\n",
        "\n",
        "Runtime:- 2hrs"
      ],
      "metadata": {
        "id": "EmSXua8xlYaW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import EarlyStoppingCallback\n",
        "\n",
        "# We use the parameters from the winning \"Config 2\"\n",
        "# Rank (r) = 64, Alpha = 128\n",
        "best_peft_config = LoraConfig(\n",
        "    r=64,\n",
        "    lora_alpha=128,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=[\"q_proj\", \"v_proj\"]\n",
        ")\n",
        "\n",
        "final_args = TrainingArguments(\n",
        "    output_dir=\"./final_model_sql\",\n",
        "    num_train_epochs=3,                     # Full training (3 epochs)\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=2,\n",
        "    learning_rate=2e-5,                     # Winning Learning Rate\n",
        "    logging_steps=25,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=100,\n",
        "    save_steps=100,\n",
        "    load_best_model_at_end=True,            # Rubric: Checkpointing\n",
        "    report_to=\"tensorboard\",                # Rubric: Logging\n",
        "    save_total_limit=2\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=dataset_dict['train'],\n",
        "    eval_dataset=dataset_dict['validation'],\n",
        "    peft_config=best_peft_config,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=512,\n",
        "    tokenizer=tokenizer,\n",
        "    args=final_args,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)] # Rubric: Callbacks\n",
        ")\n",
        "\n",
        "print(\"Starting Final Training...\")\n",
        "trainer.train()\n",
        "trainer.save_model(\"./final_best_model\")\n",
        "print(\"Training Complete!\")"
      ],
      "metadata": {
        "id": "fJhclR9dY14d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Model Evaluation & Error Analysis\n",
        "We evaluate the model using **Exact Match Accuracy** on the unseen Test Set.\n",
        "\n",
        "**Metric Definition:**\n",
        "$$\\text{Accuracy} = \\frac{\\text{Count(Generated SQL == Reference SQL)}}{\\text{Total Test Samples}}$$\n",
        "\n",
        "**Qualitative Analysis Plan:**\n",
        "We will analyze specific failure modes to distinguish between:\n",
        "* **Syntax Errors:** Invalid SQL (e.g., missing brackets).\n",
        "* **Logic Errors:** Correct syntax but wrong intent (e.g., `AVG` instead of `SUM`).\n",
        "* **Hallucinations:** Inventing columns not present in the schema context."
      ],
      "metadata": {
        "id": "v649gZdKldyM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# 1. Switch Model to Evaluation Mode\n",
        "model.eval()\n",
        "\n",
        "def normalize_sql(sql_text):\n",
        "    \"\"\"Cleans up SQL for fairer comparison.\"\"\"\n",
        "    if not sql_text: return \"\"\n",
        "    # Remove newlines and extra spaces\n",
        "    sql_text = sql_text.replace(\"\\n\", \" \").strip()\n",
        "    # Lowercase everything for comparison (SELECT == select)\n",
        "    sql_text = \" \".join(sql_text.lower().split())\n",
        "    # Remove trailing semicolons\n",
        "    return sql_text.rstrip(\";\")\n",
        "\n",
        "def generate_sql(query, context, model, tokenizer):\n",
        "    prompt = f\"\"\"### Instruction:\n",
        "You are a powerful SQL expert. Convert the following natural language question into a SQL query using the provided context.\n",
        "\n",
        "### Question:\n",
        "{query}\n",
        "\n",
        "### Context:\n",
        "{context}\n",
        "\n",
        "### Response:\n",
        "\"\"\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=200,          # Increased to prevent cut-offs\n",
        "        do_sample=False,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        repetition_penalty=1.1       # Slight penalty to stop loops\n",
        "    )\n",
        "\n",
        "    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # CRITICAL FIX: The model sometimes babbles after the answer.\n",
        "    # We take the text AFTER \"### Response:\"\n",
        "    raw_response = decoded.split(\"### Response:\")[-1].strip()\n",
        "\n",
        "    # We also cut it off if it starts hallucinating new sections like \"###\" or \"Useful links\"\n",
        "    clean_response = raw_response.split(\"###\")[0].split(\"\\n\\n\")[0].strip()\n",
        "\n",
        "    return clean_response\n",
        "\n",
        "# 2. Run Inference on Test Set\n",
        "# We increase the sample size to 50 to get a better statistical representation\n",
        "test_samples = dataset_dict['test'].select(range(50))\n",
        "results_list = []\n",
        "\n",
        "print(\"Running Inference on Test Set (Improved)...\")\n",
        "for sample in tqdm(test_samples):\n",
        "    generated_sql = generate_sql(sample['question'], sample['context'], model, tokenizer)\n",
        "\n",
        "    norm_gen = normalize_sql(generated_sql)\n",
        "    norm_exp = normalize_sql(sample['answer'])\n",
        "\n",
        "    # Strict Match: Exact string match\n",
        "    is_exact = norm_gen == norm_exp\n",
        "\n",
        "    # Soft Match: Did we at least get the right table and columns? (Good for analysis)\n",
        "    is_partial = (sample['answer'].split()[1] in generated_sql)\n",
        "\n",
        "    results_list.append({\n",
        "        \"Question\": sample['question'],\n",
        "        \"Expected\": sample['answer'],\n",
        "        \"Generated\": generated_sql,\n",
        "        \"Exact Match\": is_exact\n",
        "    })\n",
        "\n",
        "# 3. Calculate Accuracy\n",
        "df = pd.DataFrame(results_list)\n",
        "accuracy = df[\"Exact Match\"].mean() * 100\n",
        "print(f\"\\nImproved Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "# 4. Display Analysis\n",
        "print(\"\\n--- ‚úÖ SUCCESS EXAMPLES ---\")\n",
        "print(df[df[\"Exact Match\"] == True][[\"Question\", \"Generated\"]].head(2).to_string(index=False))\n",
        "\n",
        "print(\"\\n--- ‚ùå ERROR ANALYSIS (For Report) ---\")\n",
        "# Show close calls where we missed by a little bit\n",
        "errors = df[df[\"Exact Match\"] == False].head(3)\n",
        "for i, row in errors.iterrows():\n",
        "    print(f\"\\nQ: {row['Question']}\")\n",
        "    print(f\"Exp: {row['Expected']}\")\n",
        "    print(f\"Gen: {row['Generated']}\")"
      ],
      "metadata": {
        "id": "575-U9BVd6sp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}